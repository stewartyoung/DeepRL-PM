{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T14:20:25.227214Z",
     "start_time": "2021-05-31T14:20:12.898163Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/stewart/miniconda3/envs/rl_pm/lib/python3.5/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "# standard packages\n",
    "from utils.pickling import pickleIt\n",
    "from utils.plotting import plotIt\n",
    "from utils.metrics import SharpeRatio, MDD\n",
    "from universal import algos\n",
    "import seaborn as sns\n",
    "from DeepRL.utils.misc import run_episodes\n",
    "import shutil\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from networks.base_networks import DeterministicActorNet, DeterministicCriticNet\n",
    "from DeepRL.component import GaussianPolicy, HighDimActionReplay, OrnsteinUhlenbeckProcess\n",
    "from DeepRL.agent import ProximalPolicyOptimization, DisjointActorCriticNet\n",
    "import gym\n",
    "from DeepRL.utils import Logger\n",
    "from agents.DDPGAgent import DDPGAgent\n",
    "from environment.config import Config\n",
    "from wrappers import TransposeHistory, ConcatStates, SoftmaxActions, DeepRLWrapper\n",
    "from environment.stock_environment import PortfolioEnv\n",
    "import argparse\n",
    "from utils.dataPreprocessing import preProcessData\n",
    "import datetime\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/stewart/miniconda3/envs/rl_pm/lib/python3.5/site-packages/matplotlib/__init__.py:1405: UserWarning: \nThis call to matplotlib.use() has no effect because the backend has already\nbeen chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\nor matplotlib.backends is imported for the first time.\n\n  warnings.warn(_use_error_msg)\n[2021-06-10 13:27:01,381] __main__ logger started.\n"
     ]
    }
   ],
   "source": [
    "# us a non interactive plotting environment for this case\n",
    "mpl.use('TkAgg')\n",
    "plt.style.use('ggplot')\n",
    "# logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)\n",
    "# models and tensorboard logging\n",
    "# save dir\n",
    "ts = datetime.datetime.utcnow().strftime('%Y%m%d_%H-%M-%S')\n",
    "save_path = './outputs/pytorch-DDPG/pytorch-DDPG-EIIE-action-crypto-%s.model' % ts\n",
    "save_path\n",
    "try:\n",
    "    os.makedirs(os.path.dirname(save_path))\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "os.sys.path.append(os.path.abspath('..'))\n",
    "os.sys.path.append(os.path.abspath('DeepRL'))\n",
    "gym.logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-define some utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T21:44:40.234073Z",
     "start_time": "2021-05-10T21:44:40.211760Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_ddpg(agent):\n",
    "    agent_type = agent.__class__.__name__\n",
    "    save_file = 'data/%s-%s-model-%s.bin' % (agent_type, config.tag, agent.task.name)\n",
    "    agent.save(save_file)\n",
    "    print(save_file)\n",
    "    \n",
    "\n",
    "def load_ddpg(agent):\n",
    "    agent_type = agent.__class__.__name__\n",
    "    save_file = 'data/%s-%s-model-%s.bin' % (agent_type, config.tag, agent.task.name)\n",
    "    new_states = pickle.load(open(save_file, 'rb'))\n",
    "    states = agent.worker_network.load_state_dict(new_states)\n",
    "\n",
    "\n",
    "def load_stats_ddpg(agent):\n",
    "    agent_type = agent.__class__.__name__\n",
    "    online_stats_file = 'data/%s-%s-online-stats-%s.bin' % (\n",
    "                    agent_type, config.tag, agent.task.name)\n",
    "    try:\n",
    "        steps, rewards = pickle.load(open(online_stats_file, 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        steps =[]\n",
    "        rewards=[]\n",
    "    df_online = pd.DataFrame(np.array([steps, rewards]).T, columns=['steps','rewards'])\n",
    "    if len(df_online):\n",
    "        df_online['step'] = df_online['steps'].cumsum()\n",
    "        df_online.index.name = 'episodes'\n",
    "    \n",
    "    stats_file = 'data/%s-%s-all-stats-%s.bin' % (agent_type, config.tag, agent.task.name)\n",
    "    try:\n",
    "        stats = pickle.load(open(stats_file, 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        stats = {}\n",
    "    df = pd.DataFrame(stats[\"test_rewards\"], columns=['rewards'])\n",
    "    if len(df):\n",
    "#         df[\"steps\"]=range(len(df))*50\n",
    "\n",
    "        df.index.name = 'episodes'\n",
    "    return df_online, df\n",
    "\n",
    "def universalPortfolioStrat(env, algo, seed=0):\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    # start the environment from the start using reset()\n",
    "    state = env.reset()\n",
    "    # unwrapped removes all wrappers the environment instance has, \n",
    "    # then step through the environment with the for loop\n",
    "    for _ in range(env.unwrapped.sim.steps):\n",
    "        history = pd.DataFrame(state[0,:,:], columns=env.unwrapped.src.asset_names)\n",
    "        # modern portfolio theory approach to universal portfolios needs cash as 1st column\n",
    "        history[\"CASH\"] = 1\n",
    "        history = history[['CASH'] + env.unwrapped.src.asset_names]\n",
    "        # some strategies need a history - history (BSF, OLMAR), \n",
    "        # others just need the previous time step - x (ONS, EG, RMR)\n",
    "        x = history.iloc[-1]\n",
    "        # portfolio weights w0 from the previous time step\n",
    "        last_weights = env.unwrapped.sim.w0\n",
    "        # fill algo object with stock returns history\n",
    "        algo.init_step(history)\n",
    "        # some strategies don't require history\n",
    "        try:\n",
    "            action = algo.step(x, last_weights, history)\n",
    "        except TypeError:\n",
    "            action = algo.step(x, last_weights)\n",
    "        action = getattr(action, 'value', action)\n",
    "        # format for universal portfolio theory strategy\n",
    "        if isinstance(action, np.matrixlib.defmatrix.matrix):\n",
    "            action = np.array(action.tolist()).T[0]\n",
    "        # take the action on the environment, observing new state and reward\n",
    "        # done=at last time step (True), o.w. (False), info=debugging dictionary\n",
    "        state, reward, done, info = env.step(action)\n",
    "        # if at last time step, break out of for loop\n",
    "        if done:\n",
    "            break\n",
    "    # make dataframe of the returns information\n",
    "    df = pd.DataFrame(env.unwrapped.infos)\n",
    "    df.index = pd.to_datetime(df['date']*1e9)\n",
    "    \n",
    "    return df['portfolio_value'], df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access some financial data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note, I used thomson reuters datastream / refinitiv data for my dissertation, but I cannot share such data in any format outside of academic use**"
   ]
  },
  {
   "source": [
    "## There is a script in the data directory for downloding DOW 30 data and the desired format of the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Preprocess the data by dropping columns with any problems"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensorboard --logdir runs/DAX\n"
     ]
    }
   ],
   "source": [
    "args_data = \"DAX\"\n",
    "# setup tensorboard logging\n",
    "from tensorboard_logger import configure, log_value\n",
    "tag = 'ddpg-' + args_data\n",
    "print('tensorboard --logdir '+\"runs/\" + args_data)\n",
    "try:\n",
    "    configure(\"runs/\" + tag)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    pass\n",
    "# import data\n",
    "problemsAll = {\"CAC40\": [], \"DAX\": [\"VONOVIA\", \"COVESTRO\", \"MTU AERO ENGINES HLDG.\", \"LINDE (FRA)\"],\n",
    "                \"FTSE100\": [\"GLENCORE\", \"EXPERIAN\", \"INTL.CONS.AIRL.GP.\", \"COCA-COLA HBC\", \"HARGREAVES LANSDOWN\", \"MONDI\",\n",
    "                            \"OCADO GROUP\", \"STANDARD LIFE ABERDEEN\", \"AUTO TRADER GROUP\", \"EVRAZ\", \"NMC HEALTH\", \"RIGHTMOVE\",\n",
    "                            \"HIKMA PHARMACEUTICALS\", \"PHOENIX GROUP HDG.\", \"POLYMETAL INTERNATIONAL\", \"TUI (LON)\",\n",
    "                            \"ROYAL DUTCH SHELL A(LON)\", \"SMURFIT KAPPA GP. (LON)\"],\n",
    "                \"NIKKEI225\": [\"RECRUIT HOLDINGS\", \"JAPAN POST HOLDINGS\", \"OTSUKA HOLDINGS\", \"DAI-ICHI LIFE HOLDINGS\",\n",
    "                                \"SOMPO HOLDINGS\", \"INPEX\", \"JXTG HOLDINGS\", \"IDEMITSU KOSAN\", \"MEIJI HOLDINGS\", \"MITSUBISHI CHM.HDG.\",\n",
    "                                \"SONY FINANCIAL HOLDINGS\", \"AOZORA BANK\", \"CONCORDIA FINANCIAL GP.\", \"DENA\", \"NIPPON PAPER INDUSTRIES\",\n",
    "                                \"SUMCO\", \"TOKYU FUDOSAN HOLDINGS\"],\n",
    "                \"SP500\": [\"FACEBOOK CLASS A\", \"VISA 'A'\", \"MASTERCARD\", \"ABBVIE\", \"PAYPAL HOLDINGS\", \"PHILIP MORRIS INTL.\",\n",
    "                            \"BROADCOM\", \"CHARTER COMMS.CL.A\", \"T-MOBILE US\", \"ZOETIS A\", \"GENERAL MOTORS\", \"INGERSOLL RAND\",\n",
    "                            \"SERVICENOW\", \"MSCI\", \"INTERCONTINENTAL EX.\", \"KINDER MORGAN\", \"HCA HEALTHCARE\", \"LAS VEGAS SANDS\",\n",
    "                            \"DELTA AIR LINES\", \"DOLLAR GENERAL\", \"KRAFT HEINZ\", \"MARATHON PETROLEUM\", \"PHILLIPS 66\",\n",
    "                            \"DISCOVER FINANCIAL SVS.\", \"IHS MARKIT\", \"TRANSDIGM GROUP\", \"TWITTER\", \"VERISK ANALYTICS CL.A\",\n",
    "                            \"AMERICAN WATER WORKS\", \"DIGITAL REALTY TST.\", \"FIRST REPUBLIC BANK\", \"FLEETCOR TECHNOLOGIES\",\n",
    "                            \"HILTON WORLDWIDE HDG.\", \"IQVIA HOLDINGS\", \"TE CONNECTIVITY\", \"LYONDELLBASELL INDS.CL.A\",\n",
    "                            \"UNITED AIRLINES HOLDINGS\", \"APTIV\", \"CHIPOTLE MEXN.GRILL\", \"FORTIVE\", \"AMERICAN AIRLINES GROUP\",\n",
    "                            \"AMERIPRISE FINL.\", \"EXPEDIA GROUP\", \"HEWLETT PACKARD ENTER.\", \"KEYSIGHT TECHNOLOGIES\",\n",
    "                            \"SYNCHRONY FINANCIAL\", \"ARISTA NETWORKS\", \"CDW\", \"CF INDUSTRIES HDG.\", \"CITIZENS FINANCIAL GROUP\",\n",
    "                            \"FORTINET\", \"LIVE NATION ENTM.\", \"PAYCOM SOFTWARE\", \"ULTA BEAUTY\", \"XYLEM\", \"ALLEGION\",\n",
    "                            \"BROADRIDGE FINL.SLTN.\", \"CAPRI HOLDINGS\", \"CELANESE\", \"CONCHO RESOURCES\", \"LAMB WESTON HOLDINGS\",\n",
    "                            \"LEIDOS HOLDINGS\", \"CBOE GLOBAL MARKETS\", \"COTY CL.A\", \"DIAMONDBACK ENERGY\", \"INVESCO\",\n",
    "                            \"MARKETAXESS HOLDINGS\", \"NIELSEN\", \"WESTERN UNION\", \"WESTROCK\", \"DISCOVERY SERIES A\",\n",
    "                            \"FORTUNE BNS.HM.& SCTY.\", \"HANESBRANDS\", \"HNTGTN.INGALLS INDS.\", \"NORWEGIAN CRUISE LINE HDG.\",\n",
    "                            \"UNDER ARMOUR A\", \"IPG PHOTONICS\", \"NEWS 'A'\", \"ALPHABET 'C'\", \"DISCOVERY SERIES C\", \"NEWS 'B'\",\n",
    "                            \"UNDER ARMOUR 'C'\", ],\n",
    "                \"TSX\": [\"CENOVUS ENERGY\", \"GIBSON ENERGY\", \"MEG ENERGY\", \"PAREX RESOURCES\", \"PRAIRIESKY ROYALTY\",\n",
    "                        \"SEVEN GENERATIONS ENERGY\", \"TOURMALINE OIL\", \"ENERFLEX WNI.\", \"SECURE ENERGY SERVICES\",\n",
    "                        \"WHITECAP RESOURCES\"]}\n",
    "problems = problemsAll[args_data]\n",
    "keep = []"
   ]
  },
  {
   "source": [
    "## Run the simulation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of index you wish to portfolio manage\n",
    "proportion_assets = [1.0, 0.5]\n",
    "# number of runs you would like of each proportion of assets\n",
    "num_repeats = 4\n",
    "# whether you wish to use GPU or not\n",
    "gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ndata: DAX gpu: False proportion_assets: 1.0 repeat_no: 0 \n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-70c7683ca966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDPGAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;31m# train agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/cs907-13/db_ai_presentation/src/portfolio-management/agents/DDPGAgent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-70c7683ca966>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                 task.state_dim, task.action_dim, non_linear=F.relu, batch_norm=False, gpu=args.gpu)\n\u001b[1;32m     39\u001b[0m             config.network_fn = lambda: DisjointActorCriticNet(\n\u001b[0;32m---> 40\u001b[0;31m                 config.actor_network_fn, config.critic_network_fn)\n\u001b[0m\u001b[1;32m     41\u001b[0m             config.actor_optimizer_fn = lambda params: torch.optim.Adam(\n\u001b[1;32m     42\u001b[0m                 params, lr=4e-5)\n",
      "\u001b[0;32m~/Projects/cs907-13/db_ai_presentation/src/portfolio-management/DeepRL/network/continuous_action_network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, actor_network_fn, critic_network_fn)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDisjointActorCriticNet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_network_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_network_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_network_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic_network_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-70c7683ca966>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             config.actor_network_fn = lambda: DeterministicActorNet(\n\u001b[0;32m---> 36\u001b[0;31m                 task.state_dim, task.action_dim, action_gate=None, action_scale=1.0, non_linear=F.relu, batch_norm=False, gpu=args.gpu)\n\u001b[0m\u001b[1;32m     37\u001b[0m             config.critic_network_fn = lambda: DeterministicCriticNet(\n\u001b[1;32m     38\u001b[0m                 task.state_dim, task.action_dim, non_linear=F.relu, batch_norm=False, gpu=args.gpu)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "for proportion in proportion_assets:\n",
    "        data = preProcessData(\"../../data/\" + args_data + \".csv\",\n",
    "                              removeStocks=problems, keepOnly=keep, proportionAssets=proportion)\n",
    "        train = int(data.shape[0]*0.8)\n",
    "        data_train = data.iloc[:train, :]\n",
    "        data_test = data.iloc[train:, :]\n",
    "        for repeat in range(num_repeats):\n",
    "            print(\"\\ndata:\", args_data, \"gpu:\", gpu, \"proportion_assets:\", proportion,\n",
    "                  \"repeat_no:\", repeat, \"\\n\")\n",
    "            # instantiate environments\n",
    "\n",
    "            def task_fn():\n",
    "                env = PortfolioEnv(df=data_train, steps=2868, output_mode='EIIE', filename=args_data+\"train\",\n",
    "                                   timestamp=ts, proportion=proportion, repeat=repeat)\n",
    "                env = TransposeHistory(env)\n",
    "                env = ConcatStates(env)\n",
    "                env = SoftmaxActions(env)\n",
    "                env = DeepRLWrapper(env)\n",
    "                return env\n",
    "\n",
    "            def task_fn_test():\n",
    "                env = PortfolioEnv(df=data_test, steps=668, output_mode='EIIE', filename=args_data+\"test\",\n",
    "                                   timestamp=ts, proportion=proportion, repeat=repeat)\n",
    "                env = TransposeHistory(env)\n",
    "                env = ConcatStates(env)\n",
    "                env = SoftmaxActions(env)\n",
    "                env = DeepRLWrapper(env)\n",
    "                return env\n",
    "\n",
    "            task = task_fn()\n",
    "            # configure agent\n",
    "            config = Config()\n",
    "            config.task_fn = task_fn\n",
    "            task = config.task_fn()\n",
    "            config.actor_network_fn = lambda: DeterministicActorNet(\n",
    "                task.state_dim, task.action_dim, action_gate=None, action_scale=1.0, non_linear=F.relu, batch_norm=False, gpu=gpu)\n",
    "            config.critic_network_fn = lambda: DeterministicCriticNet(\n",
    "                task.state_dim, task.action_dim, non_linear=F.relu, batch_norm=False, gpu=gpu)\n",
    "            config.network_fn = lambda: DisjointActorCriticNet(\n",
    "                config.actor_network_fn, config.critic_network_fn)\n",
    "            config.actor_optimizer_fn = lambda params: torch.optim.Adam(\n",
    "                params, lr=4e-5)\n",
    "            config.critic_optimizer_fn = lambda params: torch.optim.Adam(\n",
    "                params, lr=5e-4, weight_decay=0.001)\n",
    "            config.replay_fn = lambda: HighDimActionReplay(\n",
    "                memory_size=600, batch_size=64)\n",
    "            config.random_process_fn = lambda: OrnsteinUhlenbeckProcess(\n",
    "                size=task.action_dim, theta=0.15, sigma=0.2, sigma_min=0.00002, n_steps_annealing=10000)\n",
    "            config.discount = 0.0\n",
    "            config.min_memory_size = 50\n",
    "            config.target_network_mix = 0.001\n",
    "            config.max_steps = 150000\n",
    "            config.max_episode_length = 3000\n",
    "            config.target_network_mix = 0.01\n",
    "            config.noise_decay_interval = 100000\n",
    "            config.gradient_clip = 20\n",
    "            config.min_epsilon = 0.1\n",
    "            config.reward_scaling = 1000\n",
    "            config.test_interval = 10\n",
    "            config.test_repetitions = 1\n",
    "            config.save_interval = 40\n",
    "            config.logger = Logger('./log', gym.logger)\n",
    "            config.tag = tag\n",
    "            agent = DDPGAgent(config)\n",
    "            # train agent\n",
    "            start = time.time()\n",
    "            agent.task._plot = agent.task._plot2 = None\n",
    "            try:\n",
    "                run_episodes(agent)\n",
    "                end = time.time()\n",
    "                print(\"Minutes to train:\", (end-start)/60)\n",
    "            except KeyboardInterrupt as e:\n",
    "                save_ddpg(agent)\n",
    "                end = time.time()\n",
    "                print(\"Minutes to train:\", (end-start)/60)\n",
    "                raise(e)\n",
    "            # view agent rewards and training\n",
    "            plt.figure()\n",
    "            df_online, df = load_stats_ddpg(agent)\n",
    "            snsplot = sns.regplot(\n",
    "                x=\"step\", y=\"rewards\", data=df_online, order=1)\n",
    "            snsplot.figure.savefig(\"plots/\" +\n",
    "                                   ts + \"_\" +\n",
    "                                   args.data+\"_\" +\n",
    "                                   \"rewards\"+\"_prop\" +\n",
    "                                   str(proportion) +\n",
    "                                   \"_repeat\"+str(repeat)+\".png\"\n",
    "                                   )\n",
    "            portfolio_return = (1+df_online.rewards[-100:].mean())\n",
    "            returns = task.unwrapped.src.data[0, :, :1]\n",
    "            market_return = (1+returns).mean()\n",
    "            # configure test environment\n",
    "            task = task_fn_test()\n",
    "            test_steps = 5000\n",
    "            env_test = task_fn_test()\n",
    "            agent.task = env_test\n",
    "            agent.config.max_episode_length = test_steps\n",
    "            agent.task.reset()\n",
    "            np.random.seed(0)\n",
    "            # run in deterministic mode, no training, no exploration\n",
    "            agent.episode(True)\n",
    "            agent.task.render('notebook')\n",
    "            agent.task.render('notebook', True)\n",
    "            df = pd.DataFrame(agent.task.unwrapped.infos)\n",
    "            df.index = pd.to_datetime(df['date']*1e9)\n",
    "            # test universal strategies\n",
    "            env = task.unwrapped\n",
    "            price_cols = [\n",
    "                col for col in df.columns if col.startswith('price')]\n",
    "            for col in price_cols:\n",
    "                df[col] = df[col].cumprod()\n",
    "            df = df[price_cols + ['portfolio_value']]\n",
    "            universalResults = pd.DataFrame()\n",
    "            algo_dict = dict(\n",
    "                BestSoFar=algos.BestSoFar(\n",
    "                    cov_window=env_test.unwrapped.src.window_length-1),\n",
    "                RMR=algos.RMR(\n",
    "                    window=env_test.unwrapped.src.window_length-1, eps=10),\n",
    "                ONS=algos.ONS(delta=0.2, beta=0.8, eta=0.2),\n",
    "                UCRP=algos.CRP()\n",
    "            )\n",
    "            for name, algo in algo_dict.items():\n",
    "                perf, info = universalPortfolioStrat(env_test, algo)\n",
    "                universalResults[name] = perf\n",
    "            Results = universalResults.join(df[\"portfolio_value\"])\n",
    "            Results.name = \"Results\"\n",
    "            Backtest = dict()\n",
    "            for strategy in Results.columns:\n",
    "                Backtest[strategy] = dict(MaximumDrawdown=MDD(Results[strategy]),\n",
    "                                          SharpeRatio=SharpeRatio(\n",
    "                    Results[strategy], freq=252),\n",
    "                    PortfolioValue=Results[strategy].iloc[-1])\n",
    "            Backtest = pd.DataFrame.from_dict(Backtest)\n",
    "            plotIt(Results, filename=args.data, timestamp=ts,\n",
    "                   proportion=proportion, repeat=repeat)\n",
    "            pickleIt(filename=args.data, timestamp=ts, Results=Results, Backtest=Backtest,\n",
    "                     proportion=proportion, repeat=repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python35664bitrlpmconda2cd6d6cc573d41b6a8326cdda8de2c0b",
   "display_name": "Python 3.5.6 64-bit ('rl_pm': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "metadata": {
   "interpreter": {
    "hash": "091f32d0d412f740a1d0aa7ae6fcd78334af8125359864308ced0395206ee466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}